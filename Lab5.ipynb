{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGUmFxbMNdR5sBAA2WrX8Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya1344/ML_Lab/blob/main/Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F01OR344Z71I"
      },
      "outputs": [],
      "source": [
        "lab5:\n",
        "\n",
        "A1:\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r\"C:\\\\Users\\Nikhil\\\\Desktop\\\\Feature Extraction using TF-IDF.xlsx\"\n",
        "data = pd.read_excel(file_path, sheet_name='Sheet1')  # Adjust the sheet name if needed\n",
        "\n",
        "# Extract the 'signal' and 'rank' feature vectors\n",
        "signal_data = data['signal']  # Adjust the column name if 'signal' is named differently in your dataset\n",
        "rank_data = data['rank']  # Adjust the column name if 'rank' is named differently in your dataset\n",
        "\n",
        "# Use 'signal' as the feature (X) and 'rank' as the target (y)\n",
        "X = signal_data.values.reshape(-1, 1)  # Reshape to make it a 2D array as expected by sklearn\n",
        "y = rank_data  # The target variable\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a linear regression model\n",
        "reg = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# Predict on the training set\n",
        "y_train_pred = reg.predict(X_train)\n",
        "\n",
        "# Output the model coefficients, intercept, and a sample of predictions\n",
        "print(f\"Model Coefficient: {reg.coef_}\")\n",
        "print(f\"Model Intercept: {reg.intercept_}\")\n",
        "print(f\"First few predictions on the training set: {y_train_pred[:5]}\")\n",
        "print(f\"First few actual target values: {y_train[:5].values}\")\n",
        "\n",
        "\n",
        "A2:\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r\"C:\\\\Users\\Nikhil\\\\Desktop\\\\Feature Extraction using TF-IDF.xlsx\"\n",
        "data = pd.read_excel(file_path, sheet_name='Sheet1')  # Adjust the sheet name if needed\n",
        "\n",
        "# Extract the 'signal' and 'rank' feature vectors\n",
        "signal_data = data['signal']  # Adjust the column name if 'signal' is named differently in your dataset\n",
        "rank_data = data['rank']  # Adjust the column name if 'rank' is named differently in your dataset\n",
        "\n",
        "# Use 'signal' as the feature (X) and 'rank' as the target (y)\n",
        "X = signal_data.values.reshape(-1, 1)  # Reshape to make it a 2D array as expected by sklearn\n",
        "y = rank_data  # The target variable\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a linear regression model\n",
        "reg = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# Predict on the training set\n",
        "y_train_pred = reg.predict(X_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_test_pred = reg.predict(X_test)\n",
        "\n",
        "# Calculate metrics for the training set\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = mse_train ** 0.5\n",
        "mape_train = (mean_absolute_error(y_train, y_train_pred) / y_train.mean()) * 100\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "# Calculate metrics for the test set\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "rmse_test = mse_test ** 0.5\n",
        "mape_test = (mean_absolute_error(y_test, y_test_pred) / y_test.mean()) * 100\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "# Output the metric values for train and test sets\n",
        "print(\"Training Set Metrics:\")\n",
        "print(f\"MSE: {mse_train}\")\n",
        "print(f\"RMSE: {rmse_train}\")\n",
        "print(f\"MAPE: {mape_train}\")\n",
        "print(f\"R²: {r2_train}\\n\")\n",
        "\n",
        "print(\"Test Set Metrics:\")\n",
        "print(f\"MSE: {mse_test}\")\n",
        "print(f\"RMSE: {rmse_test}\")\n",
        "print(f\"MAPE: {mape_test}\")\n",
        "print(f\"R²: {r2_test}\")\n",
        "\n",
        "\n",
        "A3:\n",
        "\n",
        "A4:\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r\"C:\\\\Users\\Nikhil\\\\Desktop\\\\Feature Extraction using TF-IDF.xlsx\"\n",
        "data = pd.read_excel(file_path, sheet_name='Sheet1')  # Adjust the sheet name if needed\n",
        "\n",
        "# Extract the 'signal' and 'rank' feature vectors (ignoring the target variable)\n",
        "X = data[['signal', 'rank']]  # Adjust column names if necessary\n",
        "\n",
        "# Split the dataset into train and test sets (though for clustering, we typically use all data)\n",
        "X_train, X_test = train_test_split(X, test_size=0.3, random_state=42)\n",
        "\n",
        "# Perform k-means clustering on the training data\n",
        "kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X_train)\n",
        "\n",
        "# Get the cluster labels for the training data\n",
        "cluster_labels_train = kmeans.labels_\n",
        "\n",
        "# Get the coordinates of the cluster centers\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "# Output the results\n",
        "print(\"Cluster Labels for Training Data:\")\n",
        "print(cluster_labels_train)\n",
        "\n",
        "print(\"\\nCluster Centers:\")\n",
        "print(cluster_centers)\n",
        "\n",
        "\n",
        "A5:\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r\"C:\\\\Users\\Nikhil\\\\Desktop\\\\Feature Extraction using TF-IDF.xlsx\"\n",
        "data = pd.read_excel(file_path, sheet_name='Sheet1')  # Adjust the sheet name if needed\n",
        "\n",
        "# Extract the 'signal' and 'rank' feature vectors (ignoring the target variable)\n",
        "X = data[['signal', 'rank']]  # Adjust column names if necessary\n",
        "\n",
        "# Split the dataset into train and test sets (though for clustering, we typically use all data)\n",
        "X_train, X_test = train_test_split(X, test_size=0.3, random_state=42)\n",
        "\n",
        "# Perform k-means clustering on the training data\n",
        "kmeans = KMeans(n_clusters=2, random_state=42).fit(X_train)\n",
        "\n",
        "# Calculate the Silhouette Score\n",
        "silhouette_avg = silhouette_score(X_train, kmeans.labels_)\n",
        "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
        "\n",
        "# Calculate the Calinski-Harabasz (CH) Score\n",
        "ch_score = calinski_harabasz_score(X_train, kmeans.labels_)\n",
        "print(f\"Calinski-Harabasz Score: {ch_score:.4f}\")\n",
        "\n",
        "# Calculate the Davies-Bouldin (DB) Index\n",
        "db_index = davies_bouldin_score(X_train, kmeans.labels_)\n",
        "print(f\"Davies-Bouldin Index: {db_index:.4f}\")\n",
        "\n",
        "A6:\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r\"C:\\\\Users\\Nikhil\\\\Desktop\\\\Feature Extraction using TF-IDF.xlsx\"\n",
        "data = pd.read_excel(file_path, sheet_name='Sheet1')  # Adjust the sheet name if needed\n",
        "\n",
        "# Extract the 'signal' and 'rank' feature vectors (ignoring the target variable)\n",
        "X = data[['signal', 'rank']]  # Adjust column names if necessary\n",
        "\n",
        "# Split the dataset into train and test sets (though for clustering, we typically use all data)\n",
        "X_train, X_test = train_test_split(X, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize lists to store the evaluation scores for different k values\n",
        "k_values = range(2, 11)  # Define the range of k values to test\n",
        "silhouette_scores = []\n",
        "ch_scores = []\n",
        "db_indices = []\n",
        "\n",
        "# Perform k-means clustering for each k value\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\").fit(X_train)\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    # Calculate the Silhouette Score\n",
        "    silhouette_avg = silhouette_score(X_train, labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "    # Calculate the Calinski-Harabasz (CH) Score\n",
        "    ch_score = calinski_harabasz_score(X_train, labels)\n",
        "    ch_scores.append(ch_score)\n",
        "\n",
        "    # Calculate the Davies-Bouldin (DB) Index\n",
        "    db_index = davies_bouldin_score(X_train, labels)\n",
        "    db_indices.append(db_index)\n",
        "\n",
        "# Plot the scores against the k values\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(k_values, silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Score vs. Number of Clusters (k)')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(k_values, ch_scores, marker='o', color='green')\n",
        "plt.title('Calinski-Harabasz Score vs. Number of Clusters (k)')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Calinski-Harabasz Score')\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(k_values, db_indices, marker='o', color='red')\n",
        "plt.title('Davies-Bouldin Index vs. Number of Clusters (k)')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Davies-Bouldin Index')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "A7:\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r\"C:\\\\Users\\\\Nikhil\\\\Desktop\\\\Feature Extraction using TF-IDF.xlsx\"\n",
        "data = pd.read_excel(file_path, sheet_name='Sheet1')  # Adjust the sheet name if needed\n",
        "\n",
        "# Extract the 'signal' and 'rank' feature vectors (ignoring the target variable)\n",
        "X = data[['signal', 'rank']]  # Adjust column names if necessary\n",
        "\n",
        "# Split the dataset into train and test sets (though for clustering, we typically use all data)\n",
        "X_train, X_test = train_test_split(X, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize a list to store the distortions (inertia)\n",
        "distortions = []\n",
        "\n",
        "# Calculate the distortions for different k values\n",
        "for k in range(2, 20):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\").fit(X_train)\n",
        "    distortions.append(kmeans.inertia_)  # Inertia measures the within-cluster sum of squares\n",
        "\n",
        "# Plot the elbow plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(2, 20), distortions, marker='o')\n",
        "plt.title('Elbow Plot for Determining Optimal k')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Distortion (Inertia)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ]
}