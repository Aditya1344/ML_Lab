{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxkj/uRCV7wqotdbUI22/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya1344/ML_Lab/blob/main/Lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0UuNvFUMaGJ9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "def load_data(file_path, sheet_name=0):\n",
        "    \"\"\"Load the dataset from an csv file.\"\"\"\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Hyperparameter tuning for Perceptron\n",
        "def tune_perceptron(X_train, y_train):\n",
        "    \"\"\"Use RandomizedSearchCV to tune Perceptron hyperparameters.\"\"\"\n",
        "    perceptron = Perceptron()\n",
        "    param_dist = {\n",
        "        'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "        'alpha': np.logspace(-4, 1, 10),\n",
        "        'max_iter': [1000, 2000, 3000]\n",
        "    }\n",
        "    random_search = RandomizedSearchCV(perceptron, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
        "    random_search.fit(X_train, y_train)\n",
        "    return random_search\n",
        "\n",
        "# Hyperparameter tuning for MLP\n",
        "def tune_mlp(X_train, y_train):\n",
        "    \"\"\"Use RandomizedSearchCV to tune MLPClassifier hyperparameters.\"\"\"\n",
        "    mlp = MLPClassifier()\n",
        "    param_dist = {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "        'activation': ['tanh', 'relu'],\n",
        "        'solver': ['adam', 'sgd'],\n",
        "        'alpha': np.logspace(-4, 1, 10),\n",
        "        'learning_rate': ['constant', 'adaptive']\n",
        "    }\n",
        "    random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
        "    random_search.fit(X_train, y_train)\n",
        "    return random_search\n",
        "\n",
        "# Main function for A2\n",
        "def run_a2_hyperparameter_tuning(file_path):\n",
        "    \"\"\"Run hyperparameter tuning for Perceptron and MLP models.\"\"\"\n",
        "    # Load data\n",
        "    data = load_data(file_path)\n",
        "    X = data.iloc[:, :-1].values  # Features\n",
        "    y = data.iloc[:, -1].values  # Target\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Tune Perceptron\n",
        "    perceptron_search = tune_perceptron(X_train, y_train)\n",
        "    print(f\"Best Perceptron Params: {perceptron_search.best_params_}\")\n",
        "    print(f\"Perceptron Test Score: {perceptron_search.score(X_test, y_test)}\")\n",
        "\n",
        "    # Tune MLP\n",
        "    mlp_search = tune_mlp(X_train, y_train)\n",
        "    print(f\"Best MLP Params: {mlp_search.best_params_}\")\n",
        "    print(f\"MLP Test Score: {mlp_search.score(X_test, y_test)}\")\n",
        "\n",
        "    # Classification report for the best models\n",
        "    y_pred_perceptron = perceptron_search.best_estimator_.predict(X_test)\n",
        "    y_pred_mlp = mlp_search.best_estimator_.predict(X_test)\n",
        "    print(\"Perceptron Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred_perceptron))\n",
        "    print(\"MLP Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred_mlp))\n",
        "\n",
        "# File path for the dataset\n",
        "file_path = '/content/Clustering.xlsx'\n",
        "run_a2_hyperparameter_tuning(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load the dataset from an Excel file.\"\"\"\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Fit and evaluate a classifier\n",
        "def evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Fit the classifier and evaluate its performance.\"\"\"\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'F1 Score': f1_score(y_test, y_pred, average='weighted')\n",
        "    }\n",
        "\n",
        "    # Try to calculate ROC AUC if classifier supports predict_proba or decision_function\n",
        "    if hasattr(clf, \"predict_proba\"):\n",
        "        try:\n",
        "            y_prob = clf.predict_proba(X_test)\n",
        "            metrics['ROC AUC'] = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "        except ValueError:\n",
        "            metrics['ROC AUC'] = \"N/A - issue with probabilities\"\n",
        "    else:\n",
        "        metrics['ROC AUC'] = \"N/A - no predict_proba\"\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Compare different classifiers\n",
        "def compare_classifiers(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Compare multiple classifiers and tabulate their results.\"\"\"\n",
        "    classifiers = {\n",
        "        'SVM': SVC(probability=True),  # Ensure SVM has probability enabled\n",
        "        'Decision Tree': DecisionTreeClassifier(),\n",
        "        'Random Forest': RandomForestClassifier(),\n",
        "        'AdaBoost': AdaBoostClassifier(),\n",
        "        'Naive Bayes': GaussianNB(),\n",
        "        'GradientBoosting': GradientBoostingClassifier()\n",
        "    }\n",
        "\n",
        "    # Evaluate each classifier and store results\n",
        "    results = {}\n",
        "    for name, clf in classifiers.items():\n",
        "        results[name] = evaluate_classifier(clf, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Print the results\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"Classifier: {name}\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"{metric}: {value}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Main function for A3 (Classifier Comparison)\n",
        "def run_a3_classifier_comparison(file_path):\n",
        "    \"\"\"Run multiple classifiers and compare their results.\"\"\"\n",
        "    # Load data\n",
        "    data = load_data(file_path)\n",
        "    X = data.iloc[:, :-1].values  # Features\n",
        "    y = data.iloc[:, -1].values  # Target\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Compare classifiers\n",
        "    compare_classifiers(X_train, y_train, X_test, y_test)\n",
        "\n",
        "# File path for the dataset\n",
        "file_path = '/content/Clustering.xlsx'\n",
        "\n",
        "# Run A3 - Classifier Comparison\n",
        "print(\"=== A3: Classifier Comparison ===\")\n",
        "run_a3_classifier_comparison(file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "XN9Elu5PaH87"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}